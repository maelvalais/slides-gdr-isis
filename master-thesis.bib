
@article{shaobing_chen_atomic_2001,
  title = {Atomic {{Decomposition}} by {{Basis Pursuit}}},
  volume = {43},
  issn = {0036-1445},
  url = {http://epubs.siam.org/doi/abs/10.1137/S003614450037906X},
  doi = {10.1137/S003614450037906X},
  abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries---stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis pursuit (BP) is a principle for decomposing a signal into an "optimal"' superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.},
  timestamp = {2016-08-16T13:11:06Z},
  number = {1},
  journal = {SIAM Rev.},
  author = {Shaobing Chen, Scott and Donoho, David and Saunders, Michael},
  urldate = {2016-07-04},
  date = {2001-01-01},
  pages = {129--159},
  file = {chen2001.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/7GEG8EV7/chen2001.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/IQ3FKC5H/Chen et al. - 2001 - Atomic Decomposition by Basis Pursuit.html:text/html}
}

@article{mairal_online_2010,
  title = {Online {{Learning}} for {{Matrix Factorization}} and {{Sparse Coding}}},
  volume = {11},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v11/mairal10a.html},
  timestamp = {2016-07-27T18:34:13Z},
  issue = {Jan},
  journal = {J. Mach. Learn. Res.},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  urldate = {2016-07-27},
  date = {2010},
  pages = {19--60},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/UU447PNZ/Mairal et al. - 2010 - Online Learning for Matrix Factorization and Spars.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/N4HB6JPZ/mairal10a.html:text/html}
}

@inproceedings{bao_fast_2013,
  title = {Fast {{Sparsity-Based Orthogonal Dictionary Learning}} for {{Image Restoration}}},
  url = {http://www.cv-foundation.org/openaccess/content_iccv_2013/html/Bao_Fast_Sparsity-Based_Orthogonal_2013_ICCV_paper.html},
  eventtitle = {Proceedings of the IEEE International Conference on Computer Vision},
  timestamp = {2016-07-28T10:46:09Z},
  author = {Bao, Chenglong and Cai, Jian-Feng and Ji, Hui},
  urldate = {2016-07-28},
  date = {2013},
  pages = {3384--3391},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/9M9MSDQV/Bao et al. - 2013 - Fast Sparsity-Based Orthogonal Dictionary Learning.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/EDPTWKGW/Bao_Fast_Sparsity-Based_Orthogonal_2013_ICCV_paper.html:text/html}
}

@unpublished{le_magoarou_learning_2014,
  title = {Learning computationally efficient dictionaries and their implementation as fast transforms},
  url = {http://arxiv.org/abs/1406.5388},
  abstract = {Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a sparse representation. The sparser the representation, the better the dictionary. The resulting dictionary is in general a dense matrix, and its manipulation can be computationally costly both at the learning stage and later in the usage of this dictionary, for tasks such as sparse coding. Dictionary learning is thus limited to relatively small-scale problems. In this paper, inspired by usual fast transforms, we consider a general dictionary structure that allows cheaper manipulation, and propose an algorithm to learn such dictionaries --and their fast implementation-- over training data. The approach is demonstrated experimentally with the factorization of the Hadamard matrix and with synthetic dictionary learning experiments.},
  timestamp = {2016-08-16T13:02:09Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.5388},
  author = {Le Magoarou, Luc and Gribonval, Rémi},
  urldate = {2016-07-28},
  date = {2014-06-20},
  keywords = {Computer Science - Learning},
  file = {arXiv\:1406.5388 PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/QANJFQAF/Magoarou et Gribonval - 2014 - Learning computationally efficient dictionaries an.pdf:application/pdf;arXiv.org Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/R5K3QSER/1406.html:text/html}
}

@article{bristow_optimization_2014,
  title = {Optimization {{Methods}} for {{Convolutional Sparse Coding}}},
  url = {http://arxiv.org/abs/1406.2407},
  abstract = {Sparse and convolutional constraints form a natural prior for many optimization problems that arise from physical processes. Detecting motifs in speech and musical passages, super-resolving images, compressing videos, and reconstructing harmonic motions can all leverage redundancies introduced by convolution. Solving problems involving sparse and convolutional constraints remains a difficult computational problem, however. In this paper we present an overview of convolutional sparse coding in a consistent framework. The objective involves iteratively optimizing a convolutional least-squares term for the basis functions, followed by an L1-regularized least squares term for the sparse coefficients. We discuss a range of optimization methods for solving the convolutional sparse coding objective, and the properties that make each method suitable for different applications. In particular, we concentrate on computational complexity, speed to \{\ensuremath{\backslash}epsilon\} convergence, memory usage, and the effect of implied boundary conditions. We present a broad suite of examples covering different signal and application domains to illustrate the general applicability of convolutional sparse coding, and the efficacy of the available optimization methods.},
  timestamp = {2016-03-03T09:38:48Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2407},
  primaryClass = {cs},
  author = {Bristow, Hilton and Lucey, Simon},
  urldate = {2016-03-03},
  date = {2014-06-09},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {arXiv\:1406.2407 PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/3P6ICVV6/Bristow et Lucey - 2014 - Optimization Methods for Convolutional Sparse Codi.pdf:application/pdf;arXiv.org Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/XVPC8V8A/1406.html:text/html}
}

@inproceedings{liu_sparse_2015,
  title = {Sparse {{Convolutional Neural Networks}}},
  url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.html},
  eventtitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  timestamp = {2016-03-03T09:39:06Z},
  author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Pensky, Marianna},
  urldate = {2016-03-03},
  date = {2015},
  pages = {806--814},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HVJAPJRA/Liu et al. - 2015 - Sparse Convolutional Neural Networks.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/KGZENXAT/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.html:text/html}
}

@article{le_magoarou_flexible_2016,
  title = {Flexible {{Multilayer Sparse Approximations}} of {{Matrices}} and {{Applications}}},
  volume = {10},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2016.2543461},
  abstract = {The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in nonconvex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising and the approximation of large matrices arising in inverse problems.},
  timestamp = {2016-08-16T13:01:49Z},
  number = {4},
  journal = {IEEE J. Sel. Top. Signal Process.},
  author = {Le Magoarou, Luc and Gribonval, Rémi},
  date = {2016-06},
  pages = {688--700},
  keywords = {Approximation algorithms,approximation theory,Complexity theory,computational complexity,computational cost,concave programming,corresponding matrix,Dictionaries,Dictionary learning,fast algorithms,flexible multilayer sparse approximations,image denoising,inverse problems,learning (artificial intelligence),linear operators,low complexity,machine learning techniques,matrix decomposition,nonconvex optimization,Optimization,Signal processing,sparse matrices,Sparse representations,transforms},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/JRGIWUEA/Magoarou et Gribonval - 2016 - Flexible Multilayer Sparse Approximations of Matri.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/43E63F58/abs_all.html:text/html}
}

@unpublished{chabiron_optimization_2016,
  title = {Optimization of a {{Fast Transform Structured}} as a {{Convolutional Tree}}},
  url = {https://hal.archives-ouvertes.fr/hal-01258514},
  abstract = {To reduce the dimension of large datasets, it is common to express each vector of this dataset using few atoms of a redundant dictionary. In order to select these atoms, many models and algorithms have been proposed, leading to state-of-the-art performances in many machine learning, signal and image processing applications. The classical sparsifying algorithms compute at each iteration matrix-vector multiplications where the matrix contains the atoms of the dictionary. As a consequence, the numerical complexity of the sparsifying algorithm is always proportional to the numerical complexity of the matrix-vector multiplication. In some applications, the matrix-vector multiplications can be computed using handcrafted fast transforms (such as the Fourier or the wavelet transforms). However, the complexity of the matrix-vector multiplications very often limits the capacities of the sparsifying algorithms. It is particularly the case when the transform is learned from the data. In order to avoid this limitation, we study a strategy to optimize convolutions of sparse kernels living on the edges of a tree. These convolutions define a fast transform (algorithmically similar to a fast wavelet transform) that can approximate atoms prescribed by the user. The optimization problem associated with the learning of these fast transforms is smooth but can be strongly non-convex. We propose in this paper a proximal alternating linearized minimization algorithm (PALMTREE) allowing Curvelet or Wavelet-packet transforms to be approximated with excellent performance. Empirically, the profile of the objective function associated with this optimization problem has a small number of critical values with large watershed. This confirms that the resulting fast transforms can be optimized efficiently, which opens many theoretical and applicative perspectives.},
  timestamp = {2016-03-08T09:38:35Z},
  author = {Chabiron, Olivier and Malgouyres, François and Wendt, Herwig and Tourneret, Jean-Yves},
  urldate = {2016-03-08},
  date = {2016-01},
  note = {working paper or preprint},
  keywords = {data analysis,data representation,Dictionary learning,Fast transform},
  file = {FTO.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/6HN3EZSA/FTO.pdf:application/pdf;HAL Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HUKK457Q/hal-01258514.html:text/html}
}

@inproceedings{pereyra_maximum--posteriori_2015,
  title = {Maximum-a-posteriori estimation with unknown regularisation parameters},
  doi = {10.1109/EUSIPCO.2015.7362379},
  abstract = {This paper presents two hierarchical Bayesian methods for performing maximum-a-posteriori inference when the value of the regularisation parameter is unknown. The methods are useful for models with homogenous regularisers (i.e., prior sufficient statistics), including all norms, composite norms and compositions of norms with linear operators. A key contribution of this paper is to show that for these models the normalisation factor of the prior has a closed-form analytic expression. This then enables the development of Bayesian inference techniques to either estimate regularisation parameters from the observed data or, alternatively, to remove them from the model by marginalisation followed by inference with the marginalised model. The effectiveness of the proposed methodologies is illustrated on applications to compressive sensing using an l1-wavelet analysis prior, where they outperform a state-of-the-art SURE-based technique, both in terms of estimation accuracy and computing time.},
  eventtitle = {2015 23rd European Signal Processing Conference (EUSIPCO)},
  timestamp = {2016-08-25T12:43:49Z},
  author = {Pereyra, M. and Bioucas-Dias, J. M. and Figueiredo, M. A. T.},
  date = {2015-08},
  pages = {230--234},
  keywords = {Approximation methods,Bayesian inference technique,Bayes methods,closed-form analytic expression,compressed sensing,compressive sensing,Computational modeling,Estimation,hierarchical Bayesian inference,hierarchical Bayesian method,I1-wavelet analysis,image processing,inference mechanisms,inverse problems,marginalised model,maximum-a-posteriori estimation,maximum-a-posteriori inference,maximum likelihood estimation,regularisation parameters,Signal processing,Signal processing algorithms,statistical signal processing,unknown regularisation parameter estimation,wavelet transforms},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VR35PAGX/Pereyra et al. - 2015 - Maximum-a-posteriori estimation with unknown regul.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/SX8NWT5D/abs_all.html:text/html}
}

@inproceedings{mailhe_shift-invariant_2008,
  location = {{Lausanne, Switzerland}},
  title = {Shift-invariant dictionary learning for sparse representations: {{Extending K-SVD}}},
  shorttitle = {Shift-invariant dictionary learning for sparse representations},
  abstract = {Shift-invariant dictionaries are generated by taking all the possible shifts of a few short patterns. They are helpful to represent long signals where the same pattern can appear several times at different positions. We present an algorithm that learns shift invariant dictionaries from long training signals. This algorithm is an extension of K-SVD. It alternates a sparse decomposition step and a dictionary update step. The update is more difficult in the shift-invariant case because of occurrences of the same pattern that overlap. We propose and evaluate an unbiased extension of the method used in K-SVD, i.e. a method able to exactly retrieve the original dictionary in a noiseless case.},
  eventtitle = {Proc. European Signal Processing Conference (EUSIPCO)},
  timestamp = {2016-08-22T13:28:59Z},
  author = {Mailhé, Boris and Lesage, Sylvain and Gribonval, Rémi and Bimbot, Frédéric and Vandergheynst, Pierre},
  date = {2008-08-25/2008-08-29},
  pages = {1--5},
  keywords = {Approximation methods,Bayes methods,Dictionaries,dictionary update,K-SVD extension,Linear programming,Matching pursuit algorithms,shift invariant dictionary learning,Signal processing algorithms,signal representation,Signal to noise ratio,singular value decomposition,sparse decomposition,sparse matrices,sparse signal representation},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/3IFHTTGS/Mailhé et al. - 2008 - Shift-invariant dictionary learning for sparse rep.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/TGR6SBTX/articleDetails.html:text/html}
}

@article{rubinstein_dictionaries_2010,
  title = {Dictionaries for {{Sparse Representation Modeling}}},
  volume = {98},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2010.2040551},
  abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.},
  timestamp = {2016-08-16T13:10:12Z},
  number = {6},
  journal = {Proc. IEEE},
  author = {Rubinstein, Ron and Bruckstein, Alfred and Elad, Michael},
  date = {2010-06},
  pages = {1045--1057},
  keywords = {Dictionaries,Dictionary learning,Displays,Harmonic analysis,Joining processes,mathematical data model,Mathematical model,Principal component analysis,redundant signal representation modeling,Sampling methods,signal approximation,Signal processing,signal representation,Signal representations,signal sampling,sparse coding,sparse representation,sparse signal representation modeling,training set,Wavelet packets,wavelet transforms},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/B5NI7R8F/Rubinstein et al. - 2010 - Dictionaries for Sparse Representation Modeling.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/QR7FHU6H/abs_all.html:text/html}
}

@article{aharon_k-svd:_2006,
  title = {K-{{SVD}}: {{An Algorithm}} for {{Designing Overcomplete Dictionaries}} for {{Sparse Representation}}},
  volume = {54},
  issn = {1053-587X},
  doi = {10.1109/TSP.2006.881199},
  shorttitle = {K-{{SVD}}},
  abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
  timestamp = {2016-08-16T13:06:15Z},
  number = {11},
  journal = {IEEE Trans. Signal Process.},
  author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  date = {2006-11},
  pages = {4311--4322},
  keywords = {Algorithm design and analysis,Atom decomposition,Basis pursuit,Clustering algorithms,codebook,Dictionaries,dictionary,Feature extraction,FOCUSS,gain-shape VQ,image coding,image data,image representation,inverse problems,Iterative algorithms,iterative method,iterative methods,K-means clustering process,K-SVD,linear transforms,matching pursuit,Matching pursuit algorithms,overcomplete dictionary,Prototypes,Pursuit algorithms,Signal design,signals sparse representation,singular value decomposition,sparse coding,sparse representation,sparsity constraints,training,transforms,vector quantization},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/JNV66I8D/Aharon et al. - 2006 - -SVD An Algorithm for Designing Overcomplete Dict.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/NVEWENIX/abs_all.html:text/html}
}

@article{chabiron_toward_2015,
  title = {Toward {{Fast Transform Learning}}},
  volume = {114},
  issn = {0920-5691, 1573-1405},
  url = {http://link.springer.com/article/10.1007/s11263-014-0771-z},
  doi = {10.1007/s11263-014-0771-z},
  abstract = {This paper introduces a new dictionary learning strategy based on atoms obtained by translating the composition of KKK convolutions with SSS-sparse kernels of known support. The dictionary update step associated with this strategy is a non-convex optimization problem. We propose a practical formulation of this problem and introduce a Gauss–Seidel type algorithm referred to as alternative least square algorithm for its resolution. The search space of the proposed algorithm is of dimension KSKSKS, which is typically smaller than the size of the target atom and much smaller than the size of the image. Moreover, the complexity of this algorithm is linear with respect to the image size, allowing larger atoms to be learned (as opposed to small patches). The conducted experiments show that we are able to accurately approximate atoms such as wavelets, curvelets, sinc functions or cosines for large values of K. The proposed experiments also indicate that the algorithm generally converges to a global minimum for large values of KKK and SSS.},
  timestamp = {2016-08-16T12:57:06Z},
  langid = {english},
  number = {2-3},
  journal = {Int. J. Comput. Vis.},
  author = {Chabiron, Olivier and Malgouyres, François and Tourneret, Jean-Yves and Dobigeon, Nicolas},
  urldate = {2016-04-01},
  date = {2015-09},
  pages = {195--216},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Imaging; Vision; Pattern Recognition and Graphics,Dictionary learning,Fast transform,Gauss–Seidel,Global optimization,Image Processing and Computer Vision,Matrix factorization,Pattern Recognition,sparse representation},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/I2SJUIJJ/Chabiron et al. - 2014 - Toward Fast Transform Learning.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/S3I7N7W9/10.html:text/html}
}

@article{sulam_trainlets:_2016,
  title = {Trainlets: {{Dictionary Learning}} in {{High Dimensions}}},
  volume = {64},
  issn = {1053-587X},
  doi = {10.1109/TSP.2016.2540599},
  abstract = {Sparse representation has shown to be a very powerful model for real world signals, and has enabled the development of applications with notable performance. Combined with the ability to learn a dictionary from signal examples, sparsity-inspired algorithms are often achieving state-of-the-art results in a wide variety of tasks. These methods have traditionally been restricted to small dimensions mainly due to the computational constraints that the dictionary learning problem entails. In the context of image processing, this implies handling small image patches. In this work we show how to efficiently handle bigger dimensions and go beyond the small patches in sparsity-based signal and image processing methods. We build our approach based on a new cropped Wavelet decomposition, which enables a multi-scale analysis with virtually no border effects. We then employ this as the base dictionary within a double sparsity model to enable the training of adaptive dictionaries. To cope with the increase of training data, while at the same time improving the training performance, we present an Online Sparse Dictionary Learning (OSDL) algorithm to train this model effectively, enabling it to handle millions of examples. This work shows that dictionary learning can be up-scaled to tackle a new level of signal dimensions, obtaining large adaptable atoms that we call Trainlets.},
  timestamp = {2016-08-16T13:09:31Z},
  number = {12},
  journal = {IEEE Trans. Signal Process.},
  author = {Sulam, Jeremias and Ophir, Boaz and Zibulevsky, Michael and Elad, Michael},
  date = {2016-06},
  pages = {3180--3193},
  keywords = {Adaptation models,Context,contourlets,cropped wavelet,Dictionary learning,Double-sparsity,image processing,K-SVD,Mathematical model,on-line learning,Signal processing algorithms,training,trainlets},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/7EAWR2ZT/Sulam et al. - 2016 - Trainlets Dictionary Learning in High Dimensions.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/95D3A9BD/articleDetails.html:text/html}
}

@article{condat_fast_2015,
  title = {Fast projection onto the simplex and the \ensuremath{\backslash}pmb \{l\}\_\ensuremath{\backslash}mathbf \{1\} ball},
  issn = {0025-5610, 1436-4646},
  url = {http://link.springer.com/article/10.1007/s10107-015-0946-6},
  doi = {10.1007/s10107-015-0946-6},
  abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an l1l1l\_1-norm ball. It can be viewed as a Gauss–Seidel-like variant of Michelot’s variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
  timestamp = {2016-05-04T13:52:48Z},
  langid = {english},
  journal = {Math. Program.},
  author = {Condat, Laurent},
  urldate = {2016-05-04},
  date = {2015-09-19},
  pages = {1--11},
  keywords = {49M30,65C60,65K05,90C25,Calculus of Variations and Optimal Control; Optimization,Combinatorics,Large-scale optimization,Mathematical Methods in Physics,Mathematics of Computing,Numerical Analysis,Simplex,Theoretical; Mathematical and Computational Physics},
  file = {condat2015.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VWJU4GMB/condat2015.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/FPZN74ZM/10.html:text/html}
}

@article{lecun_deep_2015,
  title = {Deep learning},
  volume = {521},
  rights = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  timestamp = {2016-07-13T16:54:14Z},
  langid = {english},
  number = {7553},
  journal = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  urldate = {2016-07-13},
  date = {2015-05-28},
  pages = {436--444},
  keywords = {Computer science,Mathematics and computing},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/35NDCD6V/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/DZ9T2ZJM/nature14539.html:text/html}
}

@article{held_validation_1974,
  title = {Validation of subgradient optimization},
  volume = {6},
  issn = {0025-5610, 1436-4646},
  url = {http://link.springer.com/article/10.1007/BF01580223},
  doi = {10.1007/BF01580223},
  abstract = {The “relaxation” procedure introduced by Held and Karp for approximately solving a large linear programming problem related to the traveling-salesman problem is refined and studied experimentally on several classes of specially structured large-scale linear programming problems, and results on the use of the procedure for obtaining exact solutions are given. It is concluded that the method shows promise for large-scale linear programming},
  timestamp = {2016-05-04T13:59:48Z},
  langid = {english},
  number = {1},
  journal = {Mathematical Programming},
  author = {Held, Michael and Wolfe, Philip and Crowder, Harlan P.},
  urldate = {2016-05-04},
  date = {1974-12},
  pages = {62--88},
  keywords = {Calculus of Variations and Optimal Control,Combinatorics,Mathematical and Computational Physics,Mathematical Methods in Physics,Mathematics of Computing,Numerical Analysis,Numerical and Computational Methods,Operation Research/Decision Theory,Optimization},
  file = {Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/7IQDQV9F/Held et al. - 1974 - Validation of subgradient optimization.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/2BMKN3FI/10.html:text/html}
}

@article{bolte_proximal_2014,
  title = {Proximal alternating linearized minimization for nonconvex and nonsmooth problems},
  volume = {146},
  issn = {1436-4646},
  url = {http://dx.doi.org/10.1007/s10107-013-0701-9},
  doi = {10.1007/s10107-013-0701-9},
  abstract = {We introduce a proximal alternating linearized minimization (PALM) algorithm for solving a broad class of nonconvex and nonsmooth minimization problems. Building on the powerful Kurdyka–Łojasiewicz property, we derive a self-contained convergence analysis framework and establish that each bounded sequence generated by PALM globally converges to a critical point. Our approach allows to analyze various classes of nonconvex-nonsmooth problems and related nonconvex proximal forward–backward algorithms with semi-algebraic problem’s data, the later property being shared by many functions arising in a wide variety of fundamental applications. A by-product of our framework also shows that our results are new even in the convex setting. As an illustration of the results, we derive a new and simple globally convergent algorithm for solving the sparse nonnegative matrix factorization problem.},
  timestamp = {2016-08-16T12:57:27Z},
  number = {1},
  journal = {Math. Program.},
  author = {Bolte, Jérôme and Sabach, Shoham and Teboulle, Marc},
  date = {2014},
  pages = {459--494},
  file = {bolte2013.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/3AWR4H4W/bolte2013.pdf:application/pdf}
}

@article{almeida_parameter_2013,
  title = {Parameter {{Estimation}} for {{Blind}} and {{Non-Blind Deblurring Using Residual Whiteness Measures}}},
  volume = {22},
  issn = {1057-7149},
  doi = {10.1109/TIP.2013.2257810},
  abstract = {Image deblurring (ID) is an ill-posed problem typically addressed by using regularization, or prior knowledge, on the unknown image (and also on the blur operator, in the blind case). ID is often formulated as an optimization problem, where the objective function includes a data term encouraging the estimated image (and blur, in blind ID) to explain the observed data well (typically, the squared norm of a residual) plus a regularizer that penalizes solutions deemed undesirable. The performance of this approach depends critically (among other things) on the relative weight of the regularizer (the regularization parameter) and on the number of iterations of the algorithm used to address the optimization problem. In this paper, we propose new criteria for adjusting the regularization parameter and/or the number of iterations of ID algorithms. The rationale is that if the recovered image (and blur, in blind ID) is well estimated, the residual image is spectrally white; contrarily, a poorly deblurred image typically exhibits structured artifacts (e.g., ringing, oversmoothness), yielding residuals that are not spectrally white. The proposed criterion is particularly well suited to a recent blind ID algorithm that uses continuation, i.e., slowly decreases the regularization parameter along the iterations; in this case, choosing this parameter and deciding when to stop are one and the same thing. Our experiments show that the proposed whiteness-based criteria yield improvements in SNR, on average, only 0.15 dB below those obtained by (clairvoyantly) stopping the algorithm at the best SNR. We also illustrate the proposed criteria on non-blind ID, reporting results that are competitive with state-of-the-art criteria (such as Monte Carlo-based GSURE and projected SURE), which, however, are not applicable for blind ID.},
  timestamp = {2016-08-19T11:03:03Z},
  number = {7},
  journal = {IEEE Trans. Image Process.},
  author = {Almeida, Mariana and Figueiredo, Mário},
  date = {2013-07},
  pages = {2751--2763},
  keywords = {blind deblurring,image deblurring,image deconvolution/deblurring,image recovery,image restoration,iterations,iterative methods,Monte Carlo-based GSURE,nonblind deblurring,optimisation,optimization problem,parameter estimation,projected SURE,regularization parameter,regularizer relative weight,residual squared norm,residual whiteness measures,stopping criteria,whiteness,whiteness-based criteria},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/RU4J54RN/Almeida et Figueiredo - 2013 - Parameter Estimation for Blind and Non-Blind Deblu.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/AHUFTBB9/abs_all.html:text/html}
}

@article{archer_bayesian/regularization_1995,
  title = {On some {{Bayesian}}/regularization methods for image restoration},
  volume = {4},
  issn = {1057-7149},
  doi = {10.1109/83.392339},
  abstract = {Methods are reviewed for choosing regularized restorations in image processing. In particular, a method developed by Galatsanos and Katsaggelos (see ibid., vol.1, p.322-336, 1992) is given a Bayesian interpretation and is compared with other Bayesian and non-Bayesian alternatives. A small illustrative example is provided and a complement is provided for the discussion of noise variance estimation of Galatsanos et al},
  timestamp = {2016-08-19T11:07:18Z},
  number = {7},
  journal = {IEEE Trans. Image Process.},
  author = {Archer, Graeme and Titterington, D. Michael.},
  date = {1995-07},
  pages = {989--995},
  keywords = {Additive noise,Bayesian interpretation,Bayesian method,Bayesian methods,Bayesian/regularization methods,Bayes methods,Helium,image processing,Image reconstruction,image restoration,Image sampling,Layout,maximum likelihood estimation,maximum likelihood method,noise variance estimation,noisy images,non-Bayesian method,Pixel,probability,regularized restorations,Sampling methods},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/6JKXWAH4/Archer et Titterington - 1995 - On some Bayesianregularization methods for image .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/JC9UR7HJ/abs_all.html:text/html}
}

@article{thompson_study_1991,
  title = {A study of methods of choosing the smoothing parameter in image restoration by regularization},
  volume = {13},
  issn = {0162-8828},
  doi = {10.1109/34.88568},
  abstract = {The method of regularization is portrayed as providing a compromise between fidelity to the data and smoothness, with the tradeoff being determined by a scalar smoothing parameter. Various ways of choosing this parameter are discussed in the case of quadratic regularization criteria. They are compared algebraically, and their statistical properties are comparatively assessed from the results of all extensive simulation study based on simple images},
  timestamp = {2016-08-19T11:07:04Z},
  number = {4},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  author = {Thompson, Alan M. and Brown, John C. and Kay, Jim W. and Titterington, D. Michael.},
  date = {1991-04},
  pages = {326--339},
  keywords = {Application software,Astronomy,Councils,data analysis,Entropy,filtering and prediction theory,Helium,image restoration,Physics,picture processing,quadratic regularization criteria,scalar smoothing parameter,Smoothing methods,statistical analysis,Statistics},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/MCCTW43G/Thompson et al. - 1991 - A study of methods of choosing the smoothing param.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/DMUW7UKV/abs_all.html:text/html}
}

@article{eldar_generalized_2009,
  title = {Generalized {{SURE}} for {{Exponential Families}}: {{Applications}} to {{Regularization}}},
  volume = {57},
  issn = {1053-587X},
  doi = {10.1109/TSP.2008.2008212},
  shorttitle = {Generalized {{SURE}} for {{Exponential Families}}},
  abstract = {Stein's unbiased risk estimate (SURE) was proposed by Stein for the independent, identically distributed (i.i.d.) Gaussian model in order to derive estimates that dominate least squares (LS). Recently, the SURE criterion has been employed in a variety of denoising problems for choosing regularization parameters that minimize an estimate of the mean-squared error (MSE). However, its use has been limited to the i.i.d. case which precludes many important applications. In this paper we begin by deriving a SURE counterpart for general, not necessarily i.i.d. distributions from the exponential family. This enables extending the SURE design technique to a much broader class of problems. Based on this generalization we suggest a new method for choosing regularization parameters in penalized LS estimators. We then demonstrate its superior performance over the conventional generalized cross validation and discrepancy approaches in the context of image deblurring and deconvolution. The SURE technique can also be used to design estimates without predefining their structure. However, allowing for too many free parameters impairs the estimate's performance. To address this inherent tradeoff, we propose a regularized SURE objective, and demonstrate its use in the context of wavelet denoising.},
  timestamp = {2016-08-19T14:27:23Z},
  number = {2},
  journal = {IEEE Trans. Signal Process.},
  author = {Eldar, Yonina C.},
  date = {2009-02},
  pages = {471--481},
  keywords = {Deconvolution,denoising problems,exponential families,Gaussian processes,image deblurring,image deconvolution,image denoising,image restoration,independent identically distributed Gaussian model,least mean squares methods,least squares,mean-squared error,MSE,MSE estimation,regularization,risk analysis,Stein unbiased risk estimate,wavelet denoising,wavelet transforms},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/TWUV9CMW/Eldar - 2009 - Generalized SURE for Exponential Families Applica.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/RTNR62XQ/abs_all.html:text/html}
}

@inproceedings{pereyra_comparing_2016,
  location = {{Hilton Budapest, Hungary}},
  title = {Comparing {{Bayesian}} models in the absence of ground truth},
  eventtitle = {2016 24rd European Signal Processing Conference (EUSIPCO)},
  timestamp = {2016-08-25T12:43:16Z},
  author = {Pereyra, Marcelo and McLaughlin, Steve},
  date = {2016-08-29/2016-09-02},
  keywords = {Bayesian inference,Computational imaging,Markov chain Monte Carlo,Model selection,Statistical signal processing},
  file = {Pereyra_EUSIPCO2016.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VETFMWIV/Pereyra_EUSIPCO2016.pdf:application/pdf}
}

@article{candes_fast_2006,
  title = {Fast {{Discrete Curvelet Transforms}}},
  volume = {5},
  issn = {1540-3459, 1540-3467},
  url = {http://epubs.siam.org/doi/abs/10.1137/05064182X},
  doi = {10.1137/05064182X},
  timestamp = {2016-05-17T09:37:19Z},
  langid = {english},
  number = {3},
  journal = {Multiscale Model. Simul.},
  author = {Candès, Emmanuel and Demanet, Laurent and Donoho, David and Ying, Lexing},
  urldate = {2016-05-17},
  date = {2006-01},
  pages = {861--899}
}

@article{herzet_exact_2013,
  title = {Exact {{Recovery Conditions}} for {{Sparse Representations}} with {{Partial Support Information}}},
  url = {http://arxiv.org/abs/1305.4008},
  abstract = {We address the exact recovery of a k-sparse vector in the noiseless setting when some partial information on the support is available. This partial information takes the form of either a subset of the true support or an approximate subset including wrong atoms as well. We derive a new sufficient and worst-case necessary (in some sense) condition for the success of some procedures based on lp-relaxation, Orthogonal Matching Pursuit (OMP) and Orthogonal Least Squares (OLS). Our result is based on the coherence "mu" of the dictionary and relaxes the well-known condition mu\ensuremath{<}1/(2k-1) ensuring the recovery of any k-sparse vector in the non-informed setup. It reads mu\ensuremath{<}1/(2k-g+b-1) when the informed support is composed of g good atoms and b wrong atoms. We emphasize that our condition is complementary to some restricted-isometry based conditions by showing that none of them implies the other. Because this mutual coherence condition is common to all procedures, we carry out a finer analysis based on the Null Space Property (NSP) and the Exact Recovery Condition (ERC). Connections are established regarding the characterization of lp-relaxation procedures and OMP in the informed setup. First, we emphasize that the truncated NSP enjoys an ordering property when p is decreased. Second, the partial ERC for OMP (ERC-OMP) implies in turn the truncated NSP for the informed l1 problem, and the truncated NSP for p\ensuremath{<}1.},
  timestamp = {2016-05-27T09:42:29Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1305.4008},
  primaryClass = {cs, math},
  author = {Herzet, C. and Soussen, C. and Idier, J. and Gribonval, R.},
  urldate = {2016-05-27},
  date = {2013-05-17},
  keywords = {Computer Science - Information Theory},
  file = {arXiv.org Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/ZHXG2KBU/Herzet et al. - 2013 - Exact Recovery Conditions for Sparse Representatio.html:text/html}
}

@article{tropp_signal_2007,
  title = {Signal {{Recovery From Random Measurements Via Orthogonal Matching Pursuit}}},
  volume = {53},
  issn = {0018-9448},
  doi = {10.1109/TIT.2007.909108},
  abstract = {This paper demonstrates theoretically and empirically that a greedy algorithm called orthogonal matching pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results, which require O(m2) measurements. The new results for OMP are comparable with recent results for another approach called basis pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.},
  timestamp = {2016-05-27T09:47:29Z},
  number = {12},
  journal = {IEEE Trans. Inf. Theory},
  author = {Tropp, J. A. and Gilbert, A. C.},
  date = {2007-12},
  pages = {4655--4666},
  keywords = {Algorithms,approximation,Basis pursuit,Blood,compressed sensing,greedy algorithm,greedy algorithms,group testing,iterative methods,Matching pursuit algorithms,Mathematics,orthogonal matching pursuit,Performance evaluation,random linear measurements,Reliability theory,Signal processing,signal recovery,sparse approximation,Testing,time-frequency analysis,Vectors},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/XGRE8C4E/Tropp et Gilbert - 2007 - Signal Recovery From Random Measurements Via Ortho.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/HI7HTUNJ/abs_all.html:text/html}
}

@article{donoho_stable_2006,
  title = {Stable recovery of sparse overcomplete representations in the presence of noise},
  volume = {52},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.860430},
  abstract = {Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.},
  timestamp = {2016-08-16T13:07:30Z},
  number = {1},
  journal = {IEEE Trans. Inf. Theory},
  author = {Donoho, David and Elad, Michael and Temlyakov, Vladimir},
  date = {2006-01},
  pages = {6--18},
  keywords = {approximation theory,Basis pursuit,Dictionaries,greedy approximation,greedy approximation algorithm,incoherent dictionary,iterative methods,Kruskal rank,Linear algebra,matching pursuit,Matching pursuit algorithms,Noise generators,Noise level,noisy data,optimal sparse decomposition,overcomplete representation,signal denoising,Signal processing,Signal processing algorithms,signal processing theory,signal representation,Signal representations,sparse overcomplete representation,sparse representation,Stability,stability,stable recovery,stepwise regression,superresolution,superresolution signal,time-frequency analysis,Vectors},
  file = {IEEE Xplore Full Text PDF:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/VDT966GT/Donoho et al. - 2006 - Stable recovery of sparse overcomplete representat.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/424IN4K7/Donoho et al. - 2006 - Stable recovery of sparse overcomplete representat.html:text/html}
}

@thesis{chabiron_apprentissage_2015,
  title = {Apprentissage d'arbres de convolutions pour la représentation parcimonieuse},
  url = {http://thesesups.ups-tlse.fr/2994/},
  abstract = {Le domaine de l'apprentissage de dictionnaire est le sujet d'attentions croissantes durant cette dernière décennie. L'apprentissage de dictionnaire est une approche adaptative de la représentation parcimonieuse de données. Les méthodes qui constituent l'état de l'art en DL donnent d'excellentes performances en approximation et débruitage. Cependant, la complexité calculatoire associée à ces méthodes restreint leur utilisation à de toutes petites images ou "patchs". Par conséquent, il n'est pas possible d'utiliser l'apprentissage de dictionnaire pour des applications impliquant de grandes images, telles que des images de télédétection. Dans cette thèse, nous proposons et étudions un modèle original d'apprentissage de dictionnaire, combinant une méthode de décomposition des images par convolution et des structures d'arbres de convolution pour les dictionnaires. Ce modèle a pour but de fournir des algorithmes efficaces pour traiter de grandes images, sans les décomposer en patchs. Dans la première partie, nous étudions comment optimiser une composition de convolutions de noyaux parcimonieux, un problème de factorisation matricielle non convexe. Ce modèle est alors utilisé pour construire des atomes de dictionnaire. Dans la seconde partie, nous proposons une structure de dictionnaire basée sur des arbres de convolution, ainsi qu'un algorithme de mise à jour de dictionnaire adapté à cette structure. Enfin, une étape de décomposition parcimonieuse est ajoutée à cet algorithme dans la dernière partie. À chaque étape de développement de la méthode, des expériences numériques donnent un aperçu de ses capacités d'approximation.},
  timestamp = {2016-08-16T13:09:47Z},
  institution = {{Université de Toulouse, Université Toulouse III - Paul Sabatier}},
  type = {{{PhD}}},
  author = {Chabiron, Olivier},
  urldate = {2016-06-21},
  date = {2015-10-08},
  file = {2015TOU30213.pdf:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/U7WNZ576/2015TOU30213.pdf:application/pdf;Snapshot:/Users/maelv/Library/Application Support/Zotero/Profiles/0ci2y6b1.default/zotero/storage/ZZVRX7GX/Chabiron - 2015 - Apprentissage d'arbres de convolutions pour la rep.html:text/html}
}


